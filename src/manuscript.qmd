# Does color convey political information?
<!--
    - Description: QMD script for empirical chapter 1
    - Updated: 2023-07-16
    - Notes:
        * Code:
        * Writing:
-->

## Introduction

::: {#fig-party-brand layout-ncol=2}

![Republicans pre-2000](assets/img/rnc_logo_pre-2000.png){#fig-republican}

![Democrats pre-2000](assets/img/dnc_logo_1960.png){#fig-democrat}

![Republicans 2023](assets/img/rnc_logo_2023.png){#fig-republican-post}

![Democrats 2023](assets/img/dnc_logo_2023.png){#fig-democrat-post}

Party logos
:::

Do the colors red and blue mean much to citizens in the context of politics? Before the 2000 Presidential Election, television networks used a variety of different colors when presenting electoral maps to visually communicate which candidate had won the electoral votes for each state. However, beginning in the 2000 election, networks consistently used the color red to indicate that George W. Bush (the Republican party's nominee) had won a state's electoral votes and the color blue to indicate that Al Gore (the Democratic party's nominee) had won a state's electoral votes [@elving_2014_npr].

The parties have seemingly adopted these brands[@williams_et-al_2022_jomp] and @fig-party-brand conveniently demonstrates this. While one may wonder, "Are news networks to credit/blame for giving the parties these colors?" or "Did news networks do this because choices in party branding was going this way anyhow?", this chapter wants to examine this from an alternative perspective that centers voters. Specifically, this chapter seeks to address the question of whether voters recognize these tendencies and whether this influences their levels of support for candidates in a variety of contexts.

One way in which voters may encourage the parties to adopt a consistent, but yet distinct, brand -- in terms of color -- is through the party's desire to capture as many voters as possible [@downs_1957_jpe]. For most voters, following politics and forming opinions on every policy position and candidate is cognitively demanding and time consuming. As a result, voters often satisfice and tend to repeatedly support the same party across elections [@campbell_et-al_1960_jws]. Building on these perspectives, decades of work suggests that voters see the parties as more than just a cluster of political elites who have formed a coalition around similar perspectives on policy, but that voters actually see these parties as a "team" or a social group by which one includes in their definition of self [@campbell_et-al_1960_jws;@mason_2018_cup]. As a result of this, partisans have a tendency to stick with candidates that clearly align themselves with one group and to distinguish themselves from the other group [@utych_2020_es].

Taking this perspective, this chapter argues that color works as a useful heuristic (mental shortcut) for the average citizen to figure out whether a political candidate is a Republican, a Democrat, or neither. Much like a sports team, we might expect that the athletes for our favorite team will be wearing the colors for the team they play for, even if we may not be familiar with a player who may have joined the team after the most recent draft or trade deadline [see @kurzban_et-al_2001_pnas;@pietraszewski_et-al_2015_c]. As a result of these connections between the party and a particular color, individuals will either report lower levels of support for a candidate that uses a color associated with the other political party and will report higher levels of support for the candidate using the colors that are associated with the individual's preferred party. I expect that these effects hold up particularly in the circumstance where voters have no substantive information about the candidate. Before bringing empirical evidence to examine these claims, the chapter elaborates on the argument and outlines a set of expectations about how color conveys group membership and what we may expect citizens' reactions are to such information.

## How do voters evaluate political candidates?

Political pscyhologists have amassed a great deal of evidence suggesting that voters are not objective consumers of political information. First, voters are cognitive misers and seek to find shortcuts to come to decisions about politics such as who to vote for [@lau_redlawsk_2001_ajps]. Second, one such shortcut that voters may use is by relying on the party of the candidate as opposed to relying on consistent ideological principles [@converse_1964_cr;@barber_pope_2019_apsr]. Third, the reason that partisanship is such a useful heuristic is that it now reflects a social identity that people use when defining who they are as opposed to thinking of their alignment with the parties as just a reflection of their current policy positions [@campbell_et-al_1960_jws;@mason_2018_cup;@groenendyk_2012_pb]. That is, someone's partisanship is expressive as opposed to a reflection of their evaluation of the performance of the policies advocated for by the party [@huddy_et-al_2015_apsr]. Evidence suggests that even those who claim they are independent are not necessarily less susceptible to this tendency to align their expressed support with a preferred party, but really are those who tend to dislike participating and reporting that they participate in "partisan politics" [@klar_krupnikov_2016_cup;@robison_2022_prq].

This literature in political psychology heavily borrows from social psychology which first argues that social groups are a highly effective and efficient way by which humans make decisions in a variety of contexts in and beyond politics [@tajfel_turner_1979_ejsp]; and that these social groups and our preferences for one group versus another are even reflected in neurobiological functions [@zink_barter_2012_ohsn]. Further, this literature borrows from the idea that we are motivated to defend our prior beliefs and stances. That is, we are not always motivated to collect and respond to accurate information, but we often have this strong bias to find ways to borrow and rely upon information that makes us feel good about pour prior positions [@kunda_1990_pb].

What this evidence suggests is that voters are highly motivated to defend prior positions and are often thinking about politics in terms of alignment with their preferred party [@groenendyk_2012_pb;@mason_2018_cup]. This evidence is often cited as the primary explanation for the emotionally charged negative reactions to the other parties [@iyengar_westwood_2015_ajps;@westwood_peterson_2022_pb]. This evidence also suggests that the two parties are central in how people evaluate politics -- regardless of whether or not an individual expresses high or low levels of partisan animosity. People often see politics in the United States as either a "Do I support the Democrats or do I support the Republicans?" and the two parties have put in significant efforts to make this a feature [@lee_2009;@clifford_2020_pb]. This distinction between the parties at the elite levels are reflected in real ideological polarization among elites [@enders_2021_jop] that even manifest as an unwillingness to talk to members of the other party on the House floor [@dietrich_2021_pa].

In summary, the parties use the colors red and blue at varying rates in contemporary politics [@williams_et-al_2022_jomp], they are polarized [@enders_2021_jop], and seek to find ways to distinguish themselves from the other party [@lee_2009]. For voters, they are receptive to such efforts among elites [@zaller_1992_cup;@leonard_et-al_2021_pnas] and often have motivations to stick with the party that they have supported regardless of ideological shifts [@barber_pope_2019_apsr]. Much of this information included in the literature discussed here thinks of it in terms of communicated policy positions. Over the next few pages, I build on these perspectives to argue that color is one type of symbology that the party and its candidates may use to quickly and efficiently communicate party aliance to potential voters.

## Building an intuition for color's effect on politics

+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+
| Hypotheses   | Expectation                                                                                                                                         |
+==============+:===================================================================================================================================================:+
| $H_1$        | People notice color                                                                                                                                 |
+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+
| $H_2$        | Red is associated with Republicans and Blue is associated with Democrats                                                                            |
+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+
| $H_3$        | Candidates using Red are more supported by Republicans; candidates using Blue are more supported by Democrats                                       |
+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+
| $H_4$        | Republicans spend less time evaluating candidates using Red; Democrats spend less time evaluating candidates using Blue                             |
+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+
| $H_5$        | Campaigns recognize the importance of color to voters' evaluations of candidates and respond strategically                                          |
+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+

: Summary of hypotheses {#tbl-hypotheses}

## Study 1

```{r}
#| label: study-1-setup

# Load some handy functons
box::use(
    data.table[...]
    , modelsummary[
        datasummary_skim
        , datasummary_balance
    ]
    , ./R/predicted_prob_bar[...]
)

# Execute the cleaning script
source("./eda/study_1/cleaning.R")
# Load fitted models
load("../data/temp/models/study_1_fitted.RData")
```

The purpose of [Study 1](#sec-study-1) is to examine whether the color choices on an almost ubiquitus form of campaign branding -- yard signs -- influence people's perceptions of political candidates. Evidence suggests that yard signs *do* matter in shaping political attitudes [@makse_et-al_2019_oup], vote intentions [@makse_et-al_2019_oup], and even electoral outcomes [@green_et-al_2016_es]. As they are simple, cheap, intrusive, and common forms of campaign branding, they provide a conservative test of the effects of color in campaign branding. Using yard signs in this study, I test the first four hypotheses I derive from the snap-judgment model. First, [Study 1](#sec-study-1) tests the claim that individuals do indeed notice the color of electoral yard signs. Next, it tests the claim that these colors that individuals detect influence the viewer's evaluations of the yard sign and the candidate represented on it. [Study 1](#sec-study-1) then tests the claim that the effects on perception are moderated by how consistent the color is with the more complex information displayed on the yard sign. Finally, [Study 1](#sec-study-1) examines whether positive information that contains consistency is processed quicker than negative or inconsistent information. That is, do partisans quickly detect and encode information from a clearly co-partisan political candidate?

### Research design

I recruit participants from Prolific.[^color_influence_pre-reg_anon-3] After providing informed consent to participate in the study, Prolific redirects subjects to Pavlovia[^color_influence_pre-reg_anon-4] and are provided with a demographics and political attitudes questionnaire. As I am concerned about priming effects introduced by these questions, while I am simultaneously concerned about bias introduced by post-treatment control [@montgomery_et-al_2018_ajps], I randomly select half of the participants to receive the questionnaire post-treatment and the other half receive it pre-treatment. This questionnaire includes common questions about the participant's ascriptive and descriptive characteristics and about the participant's political ideology, partisan identification, interest in politics, patriotism, and political knowledge.

[^color_influence_pre-reg_anon-3]: I pay subjects a rate of \$12.00 per hour. On top of the price per participant, Prolific charges a 30% servicing fee.

[^color_influence_pre-reg_anon-4]: Pavolovia allows for researchers to host and run open source experiments for about \$0.20 per participant (to cover their server costs). I use it primarily to integrate the JavaScript components from the jsPsych package [@jspsych] for my experimental design.

I include those questions in the questionnaire due to expectations that they may act as confounds in my hypotheses. As political knowledge intertwines with the strength to which an individual identifies with a political party [@delli-carpini_keeter_1996_yup], I expect that political knowledge is an important confounder in my tests of $H_2$, $H_3$, and $H_4$. Therefore, I include a standard battery (the American National Election Study's battery) for assessing political knowledge.[^color_influence_pre-reg_anon-5] As political knowledge is shaped by levels of interest in politics as well [@delli-carpini_keeter_1996_yup], I include the American National Election Study's question to assess levels of self-reported interest in politics. I additionally include some questions collecting information on participants' ascriptive and descriptive characteristics such as age, education, gender identity, and racial identity, as a number of these are correlates with partisan identification [see @campbell_et-al_1960_jws; @mason_2018_cup].

[^color_influence_pre-reg_anon-5]: A table including the wording of this, and all, measures are included in the Supplementary Information.

Additionally, I include a question about the respondent's sex assigned at birth and about whether they have received a diagnosis of any color blindness. As some individuals may possess undiagnosed colorblindness, asking about their sex assists in covariate balance. I additionally include an open-ended question asking participants to describe their "first memory of a political event." The use of an open-ended question helps provide an attention check and identify duplicated responses for those spoofing IP addresses with a VPN [see @kennedy_et-al_2021_poq]. Given estimates using these exclusion criteria, I would expect that upwards of 40% of my original sample will fail these attention checks [@kennedy_et-al_2021_poq]; thus, the large sample size.

I then present participants with an instruction screen informing them of the task for the experiment. In the first trial of the experiment, participants I randomly presented participants with two of three possible yard signs, one at a time. These yard signs are simple with the text "Riley Ready to Lead" and a solid background color of either "Republican Red," "Democratic Blue," or White.[^color_influence_pre-reg_anon-6] There is an added component to this, however.

[^color_influence_pre-reg_anon-6]: See the supplementary information to view all of the stimuli used in [Study 1](#sec-study-1).

Rather than use eye-tracking devices and software, I instead use Mouseview.js [see @anwyl-irvine_et-al_2022_brm], which either blocks out or blurs a large portion of the participant's screen and encourages them to move their mouse to view different parts of the screen in isolation. As the participants move their cursor around the screen, it tracks the coordinates of the cursor along with the "dwell" time of the cursor in that particular coordinate. One primary benefit of Mouseview.js is that it allows researchers to field their experiments outside of a lab-based setting -- while providing results that robustly correlate with the results from a design employing eye-tracking hardware [@anwyl-irvine_et-al_2022_brm]. This allows researchers to rely less on convenience samples, which are common with eye-tracking studies. For my design, I am particularly concerned about reliance on a convenience sample due to variations in participants' ability to detect and process color in the U.S. population relative to a student sample. This means that Mouseview.js is a handy tool for my study. A published pilot version of the experiment used in [Study 1](#sec-study-1) on [Pavlovia](https://run.pavlovia.org/damoncroberts/diss_ch_1_pre-reg/?__pilotToken=c74d97b01eae257e44aa9d5bade97baf&__oauthToken=9a645f267f9b83c8ebb48e60093bae5a746da56a6c824b8343389a9329fa2365).

When viewing each yard sign in the first trial, there is a blur over a substantial portion of the screen. At any given point in time, participants can view only 8% of the image without an obstruction, which simulates the observation that we typically foveate on about 8% of our available visual field at any given time [@wedel_pieters_2008_rmr].[^color_influence_pre-reg_anon-7] Participants move their cursor to explore the yard sign. I allot 5000 ms to perform the exploration until the image goes away to encourage a consistent and short duration to explore the image.[^color_influence_pre-reg_anon-8] After exploring each image, I asked participants what colors were on the yard sign and whether they felt that the candidate represented on the yard sign was a Democrat, a Republican, or Neither. After viewing both images, I ask subjects to indicate their preference among the two images. To ensure that participants have a standardized initial placement of their cursor, I display a blank page before viewing the yard sign that requires participants to click a "Next" button. Immediately after clicking "Next," participants are shown the yard sign. The goal is to ensure that variation in where participants explore the image is not dependent on a non-standard starting point for their cursor. I additionally utilize a gaussian blur for the overlay of the image rather than a solid overlay obstructing the participant's view. This gaussian blur allows participants to see a blurred visual field beyond the cursor. This allows participants to see enough to take purposeful action to explore blurred parts of the image that attract them [@anwyl-irvine_et-al_2022_brm]. The use of the gaussian blur requires that participants use a web browser other than Safari because of a known issue [@anwyl-irvine_et-al_2022_brm]. This will require participants to either switch browsers or to not participate in the study if they are using Safari at the time they are recruited by Prolific to participate in the study. As this requirement is enforced *before* joining the study, this should not have an effect on the number of excluded participants from my original sample.

[^color_influence_pre-reg_anon-7]: I include a screenshot providing an example of what the participants are able to see with the blur included in the supplementary materials.

[^color_influence_pre-reg_anon-8]: In marketing research, some studies give participants about 6000 milliseconds in eye-tracking studies to examine a brand and to formulate an intention to purchase a product or not [@wedel_pieters_2008_rmr]. With Mouseview.js, a study examining the tool's correlation with optical responses to viewing disgust-and-pleasure-evoking images uses 10000 milliseconds; but is intended to be an extended amount of time [@anwyl-irvine_et-al_2022_brm].

There are three more trials that are much like the first trial. What is different between the two other trials is that I vary the amount of color that is on the yard signs (trials 2 and 3). I provide more textual information that deviates from the association of Republicans with red and Democrats with blue (trial 4). Examples of all of these yard signs are included in the supplementary materials.

@tbl-study-1-descriptives presents some characteristics of the sample on the whole. 

```{r}
#| label: tbl-study-1-descriptives
#| tbl-cap: Descriptive statistics of Prolific sample

df_demographics_subset <- list_df[["cleaned"]][
    , .(
        Age = age
        , `Color blind` = color_blind_dummy
        , Female = female_sex
        , White = white
        , Hispanic = hispanic
        , Black = black
        , `Party ID` = pid_7
    )
] |> 
as.data.frame()

datasummary_skim(
    df_demographics_subset
    , histogram = FALSE
    , notes = c(
      "Data source: Pavlovia."
      , "Sample characteristics."
    )
)
```

### Do individuals notice color in political branding?

To address this first question, I use two measures of a participant's attention toward the colors on the yard sign. I collected the first measure through a question posed to the participant after viewing each yard sign, "what color was the yard sign?". The second measure is more implicit than the first: it accounts for the time someone's mouse hovered over the non-text elements of the yard sign relative to how long their mouse hovered over the text. The self-reported measure allows us to examine the conscious detection of color, while the more implicit measure allows us to examine where individuals' attention goes: toward the color or the text.

This is primarily a descriptive exercise, and I intend to compare the differences in measures between the "non-partisan" (white) yard signs and the partisan (red and blue) yard signs. I will examine the differences in measures between the partisan yard signs among self-identified partisan respondents. I additionally include additional models that interact partisanship with age and political knowledge. As the consistent usage by the parties to use the colors red and blue did not occur until the 2000 presidential election, those that were in their early adult years during that time experienced partisan politics where colors were not a strong cue. I additionally expect that those who are more politically knowledgeable should rely on these partisan cues more as they tend to be stronger partisans [@delli-carpini_keeter_1996_yup].

```{r}
#| label: tbl-h1-trial-1
#| tbl-cap: Average time between cursor movements (Trial 1)
df_movement_trial_1 <- list_df[["cleaned"]][
  , .(
    trial_1_difference_Time
    , trial_1_stimulus
    , trial_1_difference_X
    , trial_1_difference_Y
  )
]
setnames(
  df_movement_trial_1
  , old = c(
    "trial_1_difference_Time"
    , "trial_1_difference_X"
    , "trial_1_difference_Y"
  )
  , new = c(
    "$\\Delta_{t}$ (milliseconds)"
    , "$\\Delta_{x}$ (pixels)"
    , "$\\Delta_{y}$ (pixels)"
  )
)

datasummary_balance(
  ~trial_1_stimulus
  , data = df_movement_trial_1
  , notes = "Data source: Prolific sample."
  , escape = FALSE
)
```

```{r}
#| label: tbl-h1-trial-2
#| tbl-cap: Average time between cursor movements (Trial 2)
df_movement_trial_2 <- list_df[["cleaned"]][
  , .(
    trial_2_difference_Time
    , trial_2_stimulus
    , trial_2_difference_X
    , trial_2_difference_Y
  )
]
setnames(
  df_movement_trial_2
  , old = c(
    "trial_2_difference_Time"
    , "trial_2_difference_X"
    , "trial_2_difference_Y"
  )
  , new = c(
    "$\\Delta_{t}$ (milliseconds)"
    , "$\\Delta_{x}$ (pixels)"
    , "$\\Delta_{y}$ (pixels)"
  )
)
modelsummary::datasummary_balance(
  ~trial_2_stimulus
  , data = df_movement_trial_2
  , notes = "Data source: Prolific sample."
  , escape = FALSE
)
```

```{r}
#| label: tbl-h1-trial-3
#| tbl-cap: Average time between cursor movements (Trial 3)
df_movement_trial_3 <- list_df[["cleaned"]][
  , .(
    trial_3_difference_Time
    , trial_3_stimulus
    , trial_3_difference_X
    , trial_3_difference_Y
  )
]
setnames(
  df_movement_trial_3
  , old = c(
    "trial_3_difference_Time"
    , "trial_3_difference_X"
    , "trial_3_difference_Y"
  )
  , new = c(
    "$\\Delta_{t}$(milliseconds)"
    , "$\\Delta_{x}$ (pixels)"
    , "$\\Delta_{y}$ (pixels)"
  )
)

modelsummary::datasummary_balance(
  ~trial_3_stimulus
  , data = df_movement_trial_3
  , notes = "Data source: Prolific sample."
  , escape = FALSE
)
```

### Do colors shape perceptions of political objects?

To address the following question of whether the colors affect perceptions of the candidate and the yard sign, I ask participants to report whether they perceived the candidate to be a partisan -- either Republican, Democrat, or as non-partisan immediately after viewing each image. Everything on the yard signs remains constant except for the color. As representations of ideology are associated with more than just political views but things like space [@mills_et-al_2016_bbr] and color [@maestre_medero_2022_pr], differences between respondents on the perceived political affiliations of the candidate should be more than "by chance" differences. However, differences occur based on the associations between red and blue with partisanship and the lack of political information that the color white conveys.

I examine differences in respondents' reported perceptions of the candidate's partisan affiliation. As participants are asked this question for each treatment they view, the estimand of interest reflects the difference in perceptions of the candidate's partisan affiliation between the yard sign that they saw averaged across respondents. I estimate two ordered logistic regression where the main independent variables of interest are two indicator variables for whether respondents received the "red yard sign" or the "blue yard sign" in each round of Trial 1. The baseline category for both of those indicator variables is if they instead saw the "white yard sign".

My defined priors for both of these models' coefficients are Normally distributed with a mean of zero and a standard deviation of one. This reflects me starting from the position that my data need to provide evidence in support of my hypotheses rather than disproving them. The priors do allow for me to express uncertainty with this as I do have a belief that there is a chance my hypotheses are correct. Both models return $\hat{R}$ values of 1 indicating convergence and I confirm model performance by reporting the distributions of predicted values implied by my posterior distribution. These posterior predictive checks are included in @fig-h-2-ppc.

@fig-h-2-predictions present the predicted probabilities of an average subject reporting the candidate was either a Democrat, Independent, or Republican when viewing the different yard signs. The error bars reflect the high density credible interval for 95% of my posterior draws. Each bar reflects the predicted probability of an outcome -- listing the candidate is either a Democrat, Independent, or Republican. When interpreting the error bars, you will want to look at the difference in the error bars *between* yard sign treatments rather than *within* treatment. We see that the models indicate statistically meaningful differences between the yard sign treatments for each outcome. That is, the red and blue yard signs did shift people's perceptions of the partisanship of the candidate relative to the white yard sign. The credible intervals of these models are reported in the appendix in @tbl-hypothesis-2-main which contains the rest of the results from these models.

Digging in a little bit more about what these perceptions were, the predicted probability that subjects perceive the candidate with a Red yard sign to be Republican is almost 70%, whereas it is only about 30% when viewing a white yard sign. When given a red yard sign, the predicted probability the candidate is marked to be an independent is less than 30% and less than 5% to be a Democrat. However, when viewing a blue yard sign, the predicted probability of a subject perceiving the candidate to be a Democrat is at about 50% whereas it is less than 20% with a white yard sign. When viewing a blue yard sign, the predicted probability of subjects perceiving the candidate to be a Republican is less than 10% and about 45% to be an Independent. The full results are reported in @tbl-hypothesis-2-main in the appendix.

```{r}
#| label: fig-h-2-predictions
#| layout-nrow: 1
#| fig-cap: The partisanship of candidates change depending on the color of the yard sign
#| fig-subcap:
#|   - Red treatment
#|   - Blue treatment
red <- predicted_prob_bar(
  fitted_model = list_fitted[["h_2"]][["red"]]
  , x_axis = "trial_1_red_stimuli"
)
blue <- predicted_prob_bar(
  fitted_model = list_fitted[["h_2"]][["blue"]]
  , x_axis = "trial_1_blue_stimuli"
  , treatment = "Blue"
)
red
blue
```

As the consistency by which the parties have used either the color red or blue has increased since the 2000's, I suspect that the age of my subjects may moderate this relationship. That is, the tendency for subjects to label red yard signs as a Republican candidate versus a different color may be stronger for those who are younger as the consistency in branding is the political environment they have been around more. I expect this particular relationship may be weaker for older subjects as they may place current branding strategies in a larger political context where this was not always the case.

I fit two ordered logistic regressions where the independent variables are indicator variables reporting whether the participant received the red (the first model) or they recieved the blue treatment (the second model). Those included in the baseline category for both models are those who received the white yard sign treatment. Since I believe that age may moderate this effect, I include an interaction term for both of these models that multiplies these treatment indicator variables with age. 

Though my pre-registered expectations were that age would moderate this effect, these models suggest that there are not substantive or statistically meaningful differences in how older and younger participants responded to these treatments. The results of these models are included in the appendix in @tbl-hypothesis-2-main.

### Do these perceptions require consistency between information types?

Another hypothesis derived from the snap-judgment model suggests that inconsistency in the yard sign's visual information will lead to more mixed perceptions of the candidate's political stances. The design of the latter trials in the study presents yard signs with mixtures of non-partisan and out-partisan colors. For example, primarily red and presumably Republican yard signs, but has some blue or white in them.

I should expect that the trials that use less "consistent" visual information demonstrate more ambivalence among respondents in their reported perceptions of the politician's positions. Specifically, I expect that participants will perceive the yard signs that have both red and blue on them as more moderate and that the higher proportion of the color red or blue among the two colors will lead respondents, on average, to be more likely to believe that the candidate leans more Republican or Democratic. This is not to say that I expect the results to change significantly, but rather that the certainty by which I can predict individuals' guesses of the candidate's partisan affiliation to be weaker.

I fit four ordered logistic regression models. Two for each trial. Where each model for each trial models whether an average subject in the study reported that the yard sign they saw in the trial was a Democrat, Indepdent, or a Republican. @tbl-hypothesis-2a located in the appendix reports the full results of these models while @fig-h-2a-ppc located in the appendix reports the posterior predictive checks of these models.

```{r}
#| label: fig-h-2a-predictions
#| layout-nrow: 2
#| fig-cap: Mixing red and blue in a yard sign increases uncertainty about partisanship
#| fig-subcap:
#|   - Trial 2 Red treatment
#|   - Trial 2 Blue treatment
#|   - Trial 3 Red treatment
#|   - Trial 3 Blue treatment
plot_trial_2_red <- predicted_prob_bar(
  fitted_model = list_fitted[["h_2a"]][["trial_2_red"]]
  , x_axis = "trial_2_red_stimuli"
)
plot_trial_2_blue <- predicted_prob_bar(
  fitted_model = list_fitted[["h_2a"]][["trial_2_blue"]]
  , x_axis = "trial_2_blue_stimuli"
  , treatment = "Blue"
)
plot_trial_3_red <- predicted_prob_bar(
  fitted_model = list_fitted[["h_2a"]][["trial_3_red"]]
  , x_axis = "trial_3_red_stimuli"
)
plot_trial_3_blue <- predicted_prob_bar(
  fitted_model = list_fitted[["h_2a"]][["trial_3_blue"]]
  , x_axis = "trial_3_blue_stimuli"
  , treatment = "Blue"
)
plot_trial_2_red
plot_trial_2_blue
plot_trial_3_red
plot_trial_3_blue
```

The results from these models offer partial support for my expectations. First, I see that yard signs using less blue are not as distinguishably Democratic as the white yard sign. In other words, voters are not meaningfully more likely to report that the yard sign with less blue on it belongs to a Democratic candidate as a white and black yard sign. So, the strength of using a blue yard sign to communicate alignment with the Democratic party weakens if you use less blue relative to white or include red on it. With red yard signs, however, we see that voters still see them to be belonging to Republican candidates even when we reduce the amount of red on the yard sign and even include blue.

One potential reason for this may be the association of the American flag when using all three colors. Particularly given the Republican party's tendency to condemn those who do not adhere to social norms about showing respect to the American flag. This, however, is untestable based on the data I did not collect for this study. So, this explanation is speculation. However, what we do see is that there are bounds by which at least the color blue on a yard sign indicates to voters that the candidate is a Democrat. What remains unclear is whether these perceptions of ownership by a partisan candidate influences people's willingness to support that candidate or to express favoritism towards it ($H_3$).

### Does the color of a yard sign explain support for a candidate?

```{r}
#| label: fig-h-3-predictions
#| layout-nrow: 1
#| fig-cap: Probability of voting for candidate with yard sign
#| fig-subcap:
#|   - Red treatment
#|   - Blue treatment
red <- predicted_prob_bar(
  fitted_model = list_fitted[["h_3"]][["red"]]
  , x_axis = "trial_1_red_stimuli"
  , treatment = "Red"
  , hypothesis = "H3"
  , x_label = "Party ID"
  , y_label = "Pr(Vote for candidate)"
  , legend_title = "Treatment"
)
blue <- predicted_prob_bar(
  fitted_model = list_fitted[["h_3"]][["blue"]]
  , x_axis = "trial_1_blue_stimuli"
  , treatment = "Blue"
  , hypothesis = "H3"
  , x_label = "Party ID"
  , y_label = "Pr(Vote for candidate)"
  , legend_title = "Treatment"
)
red
blue
```

### Do partisans process co-partisan branding faster?

The other analytical tasks do not examine expectations derived from the motivated reasoning portion of the model. That is, do people process in-group information faster than out-group information? There is evidence of these tendencies in political circumstances [@lodge_taber_2013_cup].

To examine whether motivated reasoning is also active with the processing of politically-relevant color, I examine the difference between the amount of time between the start of viewing a stimulus and clicking "Next" to stop viewing the stimulus among those who were viewing a presumed co-partisan yard sign relative to those viewing a presumed out-partisan yard sign. As motivated reasoning tends to be more prevalent among strong partisans and those highly knowledgeable and interested in politics [@lodge_taber_2013_cup], I control for responses to the pre-treatment knowledge, interest, and partisan strength batteries.

## Study 2